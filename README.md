## Natural Language Processing through Deep Learning

Objective: To buuld a knowledge distillation student network of BERT.

NLP for computer is defined as a way to decipher, analyse, and understand a human language. Disparate tasks involved in NLP:

- Parts of speech tagging.
- Parsing sentences.
- Named-Entity Recognition
- Semantic Role Labeling
- Sentiment Classification
- Machine Translation
- Question Answering
- Dialogue Systems
- Contextual Embeddings


##### Computers understanding words
[Theory behind representation of words as vectors](https://en.wikipedia.org/wiki/Distributional_semantics)
- One hot encoding of vectors (Word embeddings)
- Word Embeddings through Word2Vec
- Character Embedding
- Contextualized Word Embeddings




### Recurrent Neural Networks
[For better understanding of RNN's](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

- Basic architecture of RNN
- LSTMs/GRU
- Sentiment analysis using RNN

<!-- ### Neural Machine Translation using RNNs -->
#### Miscellaneous: 

- Attention mechanism
- Sequence transduction model
- Transformer

##### To be covered:

- Recrusive Routing Networks
- [BERT]()
- XLNET
- Knowledge Distillation for sequence models.