{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [NLTK](https://www.nltk.org/)\n",
    "##### The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language.  (From Wiki)\n",
    "`conda install -c anaconda nltk`\n",
    "\n",
    "Corpus is collection of human language senteces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"The Eastern Conference champion Cleveland Cavaliers defeated the defending NBA champion and Western Conference champion Golden State Warriors 4–3 in a rematch of the 2015 NBA Finals. Golden State, which earned home-court advantage with setting the NBA regular season wins record (73–9), jumped to a 2–0 lead in the series while recording the largest combined margin of victory (48) through two games in NBA Finals history. Cleveland returned home and responded with a 120–90 win in Game 3, but the Warriors won Game 4 to take a 3–1 series lead. The Cavaliers won the next three games to become the first team in Finals history to successfully overcome a 3–1 deficit. It also marked the first time since 1978 that Game 7 was won by the road team. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import nltk and download necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` can help us remove stopwords that doesn't contribute much to the semantics of the sentence(s).\n",
    "\n",
    "### Now let's create Vocabulary of our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['win', 'two', 'State', 'team', 'road', 'regular', 'home', '7', 'history', 'Golden', 'Finals', 'games', 'Eastern', 'take', 'jumped', 'earned', 'victory', '2015', 'responded', 'Warriors', 'next', 'Cavaliers', 'successfully', 'first', 'rematch', 'time', 'deficit', 'Cleveland', 'series', 'defeated', '4', 'defending', '48', ',', '(', 'champion', 'Western', 'also', 'Conference', 'largest', 'become', '1978', 'record', '.', 'advantage', '3–1', 'season', '120–90', 'NBA', 'It', 'lead', 'Game', 'returned', '4–3', '2–0', 'combined', 'setting', 'wins', '73–9', 'overcome', 'margin', 'three', 'home-court', 'The', 'marked', '3', 'recording', ')', 'since']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = word_tokenize(sentences)\n",
    "#We only want unique tokens\n",
    "unique_word_tokens = set(word_tokens)\n",
    "listofwords = [w for w in unique_word_tokens if not w in stop_words]  \n",
    "print(listofwords) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets build one-hot encoded representations of each word from a sample test sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eastern  :  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Cleveland  :  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Conference  :  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Cavaliers  :  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "champion  :  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test = \"Cleveland Cavaliers are the Eastern Conference champion\"\n",
    "test = set(word_tokenize(test))\n",
    "test = [w for w in test if not w in stop_words]\n",
    "for i in range(0, len(test)):\n",
    "    one_hot = listofwords.index(test[i])\n",
    "    vector = np.zeros(len(listofwords))\n",
    "    vector[one_hot] = 1\n",
    "    print(test[i], \" : \", vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
