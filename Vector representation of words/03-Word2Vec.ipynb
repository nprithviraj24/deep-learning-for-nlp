{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec is a neural network approach to create word embeddings.\n",
    "\n",
    "This model was developed by Tomas Mikolov in 2013 at Google.\n",
    "\n",
    "### Basic intuition:\n",
    "> Word Embeddings can loosely be called as word vectors which are form of distributed representations. These word embeddings (which are word vectors) carry much more meaning than one-hot encoded representation of a word.\n",
    ">> Intuitively, we introduce some dependence of one word on the other words. The words in context of this word would get a greater share of this dependence. In one hot encoding representations, all the words are independent of each other.\n",
    "\n",
    "$D << |V| $ \n",
    "where $D$ is the number of dimensions in vector space and $|V|$ is the size of the vocabulary. In constrast to one-hot encoding where each unique word analogous to dimension in the vector space, the dimensions in word2vec model represents semantic characteristics (it is believed so).\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The objective of Word2Vec is to generate vector representations of words that carry semantic meanings for further NLP tasks. Each word vector is typically several hundred dimensions and each unique word in the corpus is assigned a vector in the space. For example, the word “happy” can be represented as a vector of 4 dimensions [0.24, 0.45, 0.11, 0.49] and “sad” has a vector of [0.88, 0.78, 0.45, 0.91]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reforming our objective\n",
    "Our objective is to have words with similar context occupy close spatial positions. Mathematically, the cosine of the angle between such vectors should be close to 1, i.e. angle close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec uses two distinct methods in order to create superior Word Embeddings.\n",
    "<strong> CBOW: Continuos Bag-of-Words </strong>\n",
    "CBOW attempts to guess the output (target word) from its neighbouring words (context words).\n",
    "\n",
    "[Architecture]()\n",
    "\n",
    "<strong> Skip-gram </strong>\n",
    "Guesses the context from a target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
