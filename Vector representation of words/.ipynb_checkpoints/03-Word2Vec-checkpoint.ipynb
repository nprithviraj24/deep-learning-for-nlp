{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec is a neural network approach to create word embeddings.\n",
    "\n",
    "This model was developed by Tomas Mikolov in 2013 at Google.\n",
    "\n",
    "### Basic intuition:\n",
    "> Word Embeddings can loosely be called as word vectors which are form of distributed representations. These word embeddings (which are word vectors) carry much more meaning than one-hot encoded representation of a word.\n",
    ">> Intuitively, we introduce some dependence of one word on the other words. The words in context of this word would get a greater share of this dependence. In one hot encoding representations, all the words are independent of each other.\n",
    "\n",
    "$D << |V| $ \n",
    "where $D$ is the number of dimensions in vector space and $|V|$ is the size of the vocabulary. In constrast to one-hot encoding where each unique word analogous to dimension in the vector space, the dimensions in word2vec model represents semantic characteristics (it is believed so).\n",
    "\n",
    "### Reforming our objective\n",
    "Our objective is to have words with similar context occupy close spatial positions. Mathematically, the cosine of the angle between such vectors should be close to 1, i.e. angle close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
